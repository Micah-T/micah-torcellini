---
title: Building a Local Newspaper Website with Eleventy
tags: ["newspaper", communicator, "web development", eleventy]
---
I recently built a website for the *[Eastford Communicator](https://eastford.news)*. Currently, the *Communicator* publishes every other month. Because it functions as a periodical, I was able to largely re-use the code for *[Westmarch](/westmarch/)* with some minor change. As with *Westmarch*, I receive articles as DOCX files and use [Pandoc](https://pandoc.org) to convert them. The editor uses folders to organize the documents and pictures together (which is very helpful!), so I use a PowerShell script to convert all the DOCX files in all subdirectories. I'm unreasonably proud of the script: 

```
`get-Childitem -Recurse -filter *docx | Foreach {pandoc (-join($_.Directory, "/", $_.Name)) -o (-join($_.Name, ".md"))}`
```

Unlike *Westmarch*, the *Communicator* provides excerpts of articles on each issue's page. I wanted to avoid having to add markers into the content, since each issue has many articles. For the most part, I followed [Jonathan Yeong's guide](https://dev.to/jonathanyeong/excerpts-with-eleventy-4od8), though I configured it as a filter rather than as a shortcode (e.g. `{% raw %}{{ post.content | excerpt }{% endraw %}}`). This also eliminates the need for error handling in case the article does not have any content, because Nunjucks will simply default to nothing. I don't know if there is a significant technical reason to write this one way or another; I just thought of it as a filter-style operation. Rather than inserting a specific content, I thought of it as modifying the insertion of the article content. In addition, this would allow me to add filters later on. 

An additional challenge with the *Communicator* is the quantity of images. In just two issues, there are already more than 45 MB of images and I am suspicious that there will possibly be even more images in later issues. If I were to commit these directly into a Git repository, it would quickly bloat the repository. I decided to use [Git LFS](https://git-lfs.com/) to minimize this problem. GitHub works seamlessly with Git LFS. GitHub LFS has a 1 GB monthly bandwidth limit, however. Cloudflare pages clones the repository every time the site builds, which quickly exceeded the bandwidth quota.[^1] I found Milkey Mouse's [Git LFS S3 Proxy project](https://github.com/milkey-mouse/git-lfs-s3-proxy), and I was able to nearly-effortlessly set up my own instance. It has worked without any issues. The Cloudflare R2 free quota is enough to support any development needs that are likely with this project. 

[^1]: Not only that, but it used the quota in the *Westmarch* repository because GitHub charges the storage and bandwidth to the parent of a forked repository, even if the parent has no LFS objects. An additional problem is that GitHub allows no automatic way to delete files from LFS storage, but having my own server allows me to delete files if necessary. That might potentially cause data loss, so a backup solution is important. If the current version of the repository is the most important, then simply backing up the local repository folder could be adequate. One could also use [Rclone](https://rclone.org/) to back up the R2/S3 bucket itself.